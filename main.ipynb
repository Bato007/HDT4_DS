{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import string, re, emoji\n",
    "\n",
    "# Limpieza de textos\n",
    "from pattern.text.en import singularize, lemma\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from transformers import AutoTokenizer,TFBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  id - a unique identifier for each tweet\n",
    "  text - the text of the tweet\n",
    "  location - the location the tweet was sent from (may be blank)\n",
    "  keyword - a particular keyword from the tweet (may be blank)\n",
    "  target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "'''\n",
    "train = pd.read_csv('./train.csv', encoding='utf8')\n",
    "test = pd.read_csv('./test.csv', encoding='utf8')\n",
    "\n",
    "cachedStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCountry(value):\n",
    "  try:\n",
    "\n",
    "    if (\n",
    "      'nan' in value or\n",
    "      'world' in value or\n",
    "      'global' in value or\n",
    "      'everywhere' in value or\n",
    "      'earth' in value or\n",
    "      'ss' in value or\n",
    "      '?' in value or\n",
    "      re.search(r\"[0-9]+\", value, re.I)\n",
    "    ):\n",
    "      return 'unknow'\n",
    "\n",
    "    if (\n",
    "      'italy' in value\n",
    "    ):\n",
    "      return 'italy'\n",
    "\n",
    "    if (\n",
    "      'india' in value or\n",
    "      'mumbai' in value\n",
    "    ):\n",
    "      return 'india'\n",
    "    \n",
    "    if (\n",
    "      'switzerland' in value or\n",
    "      'geneva' in value\n",
    "    ):\n",
    "      return 'switzerland'\n",
    "    \n",
    "    if (\n",
    "      'china' in value or\n",
    "      'hong kong' in value\n",
    "    ):\n",
    "      return 'china'\n",
    "\n",
    "    if (\n",
    "      'nigeria' in value or\n",
    "      'lagos' in value\n",
    "    ):\n",
    "      return 'nigeria'\n",
    "\n",
    "    if (\n",
    "      'japan' in value or\n",
    "      'tokyo' in value\n",
    "    ):\n",
    "      return 'japan'\n",
    "\n",
    "    if (\n",
    "      'ontario' in value or\n",
    "      'canada' in value or\n",
    "      'toronto' in value or\n",
    "      'calgary' in value or\n",
    "      'alberta' in value or\n",
    "      re.search(r\"ab$\", value, re.I) or\n",
    "      re.search(r\"bc$\", value, re.I)\n",
    "    ):\n",
    "      return 'canada'\n",
    "\n",
    "    if (\n",
    "      'uk' == value or\n",
    "      'united kingdom' in value or\n",
    "      'kingdom' in value or\n",
    "      'british' in value or\n",
    "      'scotland' in value or\n",
    "      'newcastle' in value or\n",
    "      'england' in value or\n",
    "      'london' in value or\n",
    "      re.search(r\"uk$\", value, re.I)\n",
    "    ):\n",
    "      return 'uk'\n",
    "\n",
    "    if (\n",
    "      'nyc' == value or\n",
    "      'nj' == value or\n",
    "      'united states' in value or\n",
    "      'new york' in value or\n",
    "      'san francisco' in value or\n",
    "      'los angeles' in value or\n",
    "      'new jersey' in value or\n",
    "      'north carolina' in value or\n",
    "      'st. louis' in value or\n",
    "      'kansas city' in value or\n",
    "      'san diego' in value or\n",
    "      'las vegas' in value or\n",
    "      'sacramento' in value or\n",
    "      'oregon' in value or\n",
    "      'michigan' in value or\n",
    "      'manchester' in value or\n",
    "      'portland' in value or\n",
    "      'texas' in value or\n",
    "      'u.s.' in value or\n",
    "      'philippines' in value or\n",
    "      'nevada' in value or\n",
    "      'us' in value or\n",
    "      'arizona' in value or\n",
    "      'lincoln' in value or\n",
    "      'wisconsin' in value or\n",
    "      'pennsylvania' in value or\n",
    "      'seattle' in value or\n",
    "      'usa' in value or\n",
    "      'washington' in value or\n",
    "      'florida' in value or\n",
    "      'chicago' in value or\n",
    "      'california' in value or\n",
    "      'nashville' in value or\n",
    "      'colorado' in value or\n",
    "      'denver' in value or\n",
    "      'cleveland' in value or\n",
    "      'atlanta' in value or\n",
    "      'massachusetts' in value or\n",
    "      'boston' in value or\n",
    "      'oklahoma' in value or\n",
    "      'tennessee' in value or\n",
    "      'liverpool' in value or\n",
    "      'phoenix' in value or\n",
    "      'baltimore' in value or\n",
    "      re.search(r\"nyc$\", value, re.I) or\n",
    "      re.search(r\"hi$\", value, re.I) or\n",
    "      re.search(r\"va$\", value, re.I) or\n",
    "      re.search(r\"ks$\", value, re.I) or\n",
    "      re.search(r\"la$\", value, re.I) or\n",
    "      re.search(r\"ak$\", value, re.I) or\n",
    "      re.search(r\"md$\", value, re.I) or\n",
    "      re.search(r\"mo$\", value, re.I) or\n",
    "      re.search(r\"wi$\", value, re.I) or\n",
    "      re.search(r\"az$\", value, re.I) or\n",
    "      re.search(r\"ga$\", value, re.I) or\n",
    "      re.search(r\"ok$\", value, re.I) or\n",
    "      re.search(r\"nj$\", value, re.I) or\n",
    "      re.search(r\"wa$\", value, re.I) or\n",
    "      re.search(r\"pa$\", value, re.I) or\n",
    "      re.search(r\"ma$\", value, re.I) or\n",
    "      re.search(r\"co$\", value, re.I) or\n",
    "      re.search(r\"oh$\", value, re.I) or\n",
    "      re.search(r\"il$\", value, re.I) or\n",
    "      re.search(r\"tn$\", value, re.I) or\n",
    "      re.search(r\"dc$\", value, re.I) or\n",
    "      re.search(r\"ca$\", value, re.I) or\n",
    "      re.search(r\"tx$\", value, re.I) or\n",
    "      re.search(r\"nc$\", value, re.I) or\n",
    "      re.search(r\"fl$\", value, re.I) or\n",
    "      re.search(r\"ny$\", value, re.I)\n",
    "    ):\n",
    "      return 'usa'\n",
    "\n",
    "    return value\n",
    "  except:\n",
    "    return 'unknow'\n",
    "\n",
    "train['location'] = train['location'].apply(lambda row: str(row).lower())\n",
    "train['location'] = train['location'].apply(lambda row: cleanCountry(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSingular(value):\n",
    "  try:\n",
    "    return singularize(value)\n",
    "  except:\n",
    "    return value\n",
    "\n",
    "def parseLemma(value):\n",
    "  try:\n",
    "    return lemma(value)\n",
    "  except:\n",
    "    return value\n",
    "\n",
    "def replaceSpace(value):\n",
    "  return str(value).replace('%20', ' ')\n",
    "\n",
    "train['keyword'] = train['keyword'].apply(lambda row: toSingular(row))\n",
    "train['keyword'] = train['keyword'].apply(lambda row: parseLemma(row))\n",
    "train['keyword'] = train['keyword'].apply(lambda row: replaceSpace(row))\n",
    "train['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHastags(value):\n",
    "  if ('#' not in value): return value\n",
    "  hashtags = re.findall(r\"#[^\\s]*\", value, re.I)\n",
    "  for hashtag in hashtags:\n",
    "    value = value.replace(hashtag, '')\n",
    "  return value\n",
    "\n",
    "def removeLinks(value):\n",
    "  if ('http' not in value): return value\n",
    "  links = re.findall(r\"http[^\\s]*\", value, re.I)\n",
    "  for link in links:\n",
    "    value = value.replace(link, '')\n",
    "  return value\n",
    "\n",
    "def removeStepWords(value):\n",
    "  return ' '.join([word for word in value.split() if word not in cachedStopWords])\n",
    "\n",
    "def extractEmojis(value):\n",
    "  items = value.split(' ')\n",
    "  emojis = ''.join(item for item in items if item in emoji.EMOJI_DATA)\n",
    "\n",
    "  if (len(emojis) > 0): print(value)\n",
    "\n",
    "  return value\n",
    "\n",
    "def sentenceToSingular(value):\n",
    "  items = value.split(' ')\n",
    "  for item in items:\n",
    "    singular = toSingular(item)\n",
    "    value = value.replace(item, singular)\n",
    "  return value\n",
    "\n",
    "def sentenceToPresent(value):\n",
    "  items = value.split(' ')\n",
    "  for item in items:\n",
    "    present = parseLemma(item)\n",
    "    value = value.replace(item, present)\n",
    "  return value\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "cleanText = []\n",
    "for index, row in train.iterrows():\n",
    "  text = row['text'][:]\n",
    "\n",
    "  # Limpiando el texto\n",
    "  text = text.lower()                         # Convierte todo a minusculas\n",
    "  text = text.replace('utc', '')              # Quita utc\n",
    "  text = text.replace('#', '')                # Quita #\n",
    "  text = text.replace('@', '')                # Quita @\n",
    "  text = removeLinks(text)                    # Quita links\n",
    "  text = extractEmojis(text)                  # Quita todos los emojis\n",
    "  text = text.translate(translator)           # Quita todos los signos de puntuacion\n",
    "  text = removeStepWords(text)                # Quita todas las step words\n",
    "  text = re.sub('  +', ' ', text)             # Quita todos los espacios de mas\n",
    "  text = sentenceToSingular(text)             # Pasa las palabras a singular\n",
    "  text = sentenceToPresent(text)              # Pasa las palabras a presente\n",
    "  \n",
    "  numbers = re.findall(r\"[0-9]\", text, re.I)\n",
    "  if (len(numbers) > 0):\n",
    "    for number in numbers:\n",
    "      if (number == '911'): continue\n",
    "\n",
    "      # Quitando numeros\n",
    "      text = text.replace(number, '')\n",
    "\n",
    "  text = removeStepWords(text)                # Quita todas las step words\n",
    "  text = re.sub('  +', ' ', text)             # Quita todos los espacios de mas\n",
    "\n",
    "  cleanText.append(text)\n",
    "\n",
    "train['text'] = cleanText[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train.iterrows():\n",
    "  print(row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['target'] == 0]['keyword'].value_counts().head(15).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['target'] == 1]['keyword'].value_counts().head(15).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disaster Tweets wordcloud \n",
    "disaster_tweets = train[train['target'] == 1]\n",
    "disaster_string = []\n",
    "for t in disaster_tweets.text:\n",
    "    disaster_string.append(t)\n",
    "disaster_string = pd.Series(disaster_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=100, background_color='white').generate(disaster_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive tweets wordcloud\n",
    "formal_tweets = train[train[\"target\"] == 0]\n",
    "formal_string = []\n",
    "for t in formal_tweets.text:\n",
    "    formal_string.append(t)\n",
    "formal_string = pd.Series(formal_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=100, background_color='white').generate(formal_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('Prediciendo tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max len of tweets\",max([len(x.split()) for x in train['text']]))\n",
    "max_length = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer(\n",
    "    text=train['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=36,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Input(shape=(max_length,), dtype=tensorflow.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_length,), dtype=tensorflow.int32, name=\"attention_mask\")\n",
    "\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[1] \n",
    "out = tensorflow.keras.layers.Dropout(0.1)(embeddings)\n",
    "\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tensorflow.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "\n",
    "y = Dense(1,activation = 'sigmoid')(out)\n",
    "    \n",
    "model = tensorflow.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    learning_rate=6e-06, # this learning rate is for bert model.\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = BinaryCrossentropy(from_logits = True)\n",
    "metric = BinaryAccuracy('accuracy'),\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "final = model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "    y = y_train,\n",
    "    epochs=4,\n",
    "    batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision y perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_accuracy_and_loss(final):\n",
    "    acc = final.history['accuracy']\n",
    "    loss = final.history['loss']\n",
    "    epochs_plot = np.arange(1, len(loss) + 1)\n",
    "    plt.clf()\n",
    "    plt.plot(epochs_plot, acc, 'r', label='Accuracy')\n",
    "    plt.plot(epochs_plot, loss, 'b:', label='Loss')\n",
    "    plt.title('VISUALIZATION OF LOSS AND ACCURACY CURVE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visual_accuracy_and_loss(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tokenizer(\n",
    "    text=test.text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=36,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.where(predicted>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = y_predicted.reshape((1,3263))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['id'] = test.id\n",
    "result['text'] = test.text\n",
    "result['target'] = y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobando las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prediction = tokenizer(\n",
    "    text=train['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=36,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict({'input_ids':X_test_prediction['input_ids'],'attention_mask':X_test_prediction['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_test = np.where(test_prediction>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_test = y_predicted_test.reshape((1,len(y_predicted_test)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predicted_val'] = y_predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(train['target'], train['predicted_val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion de prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functionToPredict(quant):\n",
    "    print('testing...', test_data[0:quant])\n",
    "\n",
    "    text_to_predict = tokenizer(\n",
    "        text=test_data,\n",
    "        add_special_tokens=True,\n",
    "        max_length=36,\n",
    "        truncation=True,\n",
    "        padding=True, \n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    test_prediction = model.predict({'input_ids':text_to_predict['input_ids'],'attention_mask':text_to_predict['attention_mask']})\n",
    "\n",
    "    y_predicted_test = np.where(test_prediction>0.5,1,0)\n",
    "\n",
    "    y_predicted_test = y_predicted_test.reshape((1,len(y_predicted_test)))[0]\n",
    "\n",
    "    for i in range(quant):\n",
    "        if (y_predicted_test[quant] == 0):\n",
    "            print('Identificado como no-desastre')\n",
    "        else:\n",
    "            print('Identificado como desastre')\n",
    "    \n",
    "    return y_predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test['text'].tolist()\n",
    "test_data[0] = 'Yesterday I went to the dentist'\n",
    "test_data[1] = 'A car crashed in front of me'\n",
    "test_data[2] = 'My kid is a bomb!'\n",
    "test_data[3] = 'The airplane had an accident'\n",
    "test_data[4] = 'Too much traffic!!!'\n",
    "test_data[5] = 'An earthquake killed 5 persons'\n",
    "test_data[6] = 'Typhoon destroyed houses'\n",
    "test_data[7] = 'A girl who died in an airplane accident fifteen years ago'\n",
    "test_data[8] = 'I am going into a panic attack'\n",
    "\n",
    "functionToPredict(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa13260b2ad5da8a92d4362f230c03eee7b3dfd5b47a73cfd4853277a201fcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
