{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import string, re, emoji\n",
    "\n",
    "# Limpieza de textos\n",
    "from pattern.text.en import singularize, lemma\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  id - a unique identifier for each tweet\n",
    "  text - the text of the tweet\n",
    "  location - the location the tweet was sent from (may be blank)\n",
    "  keyword - a particular keyword from the tweet (may be blank)\n",
    "  target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "'''\n",
    "train = pd.read_csv('./train.csv', encoding='utf8')\n",
    "\n",
    "cachedStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].value_counts().head(10).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSingular(value):\n",
    "  try:\n",
    "    return singularize(value)\n",
    "  except:\n",
    "    return value\n",
    "\n",
    "def parseLemma(value):\n",
    "  try:\n",
    "    return lemma(value)\n",
    "  except:\n",
    "    return value\n",
    "\n",
    "def replaceSpace(value):\n",
    "  return str(value).replace('%20', ' ')\n",
    "\n",
    "train['keyword'] = train['keyword'].apply(lambda row: toSingular(row))\n",
    "train['keyword'] = train['keyword'].apply(lambda row: parseLemma(row))\n",
    "train['keyword'] = train['keyword'].apply(lambda row: replaceSpace(row))\n",
    "train['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHastags(value):\n",
    "  if ('#' not in value): return value\n",
    "  hashtags = re.findall(r\"#[^\\s]*\", value, re.I)\n",
    "  for hashtag in hashtags:\n",
    "    value = value.replace(hashtag, '')\n",
    "  return value\n",
    "\n",
    "def removeLinks(value):\n",
    "  if ('http' not in value): return value\n",
    "  links = re.findall(r\"http[^\\s]*\", value, re.I)\n",
    "  for link in links:\n",
    "    value = value.replace(link, '')\n",
    "  return value\n",
    "\n",
    "def removeStepWords(value):\n",
    "  return ' '.join([word for word in value.split() if word not in cachedStopWords])\n",
    "\n",
    "def extractEmojis(value):\n",
    "  items = value.split(' ')\n",
    "  emojis = ''.join(item for item in items if item in emoji.EMOJI_DATA)\n",
    "\n",
    "  if (len(emojis) > 0): print(value)\n",
    "\n",
    "  return value\n",
    "\n",
    "def sentenceToSingular(value):\n",
    "  items = value.split(' ')\n",
    "  for item in items:\n",
    "    singular = toSingular(item)\n",
    "    value = value.replace(item, singular)\n",
    "  return value\n",
    "\n",
    "def sentenceToPresent(value):\n",
    "  items = value.split(' ')\n",
    "  for item in items:\n",
    "    present = parseLemma(item)\n",
    "    value = value.replace(item, present)\n",
    "  return value\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "cleanText = []\n",
    "for index, row in train.iterrows():\n",
    "  text = row['text'][:]\n",
    "\n",
    "  # Limpiando el texto\n",
    "  text = text.lower()                         # Convierte todo a minusculas\n",
    "  text = text.replace('#', '')                # Quita #\n",
    "  text = text.replace('@', '')                # Quita @\n",
    "  text = removeLinks(text)                    # Quita links\n",
    "  text = text.translate(translator)           # Quita todos los signos de puntuacion\n",
    "  text = removeStepWords(text)                # Quita todas las step words\n",
    "  text = extractEmojis(text)                  # Quita todos los emojis\n",
    "  text = re.sub(' +', ' ', text)              # Quita todos los espacios de mas\n",
    "  text = sentenceToSingular(text)             # Pasa las palabras a singular\n",
    "  text = sentenceToPresent(text)              # Pasa las palabras a presente\n",
    "  \n",
    "  numbers = re.findall(r\"[0-9]+\", text, re.I)\n",
    "  if (len(numbers) > 0):\n",
    "    for number in numbers:\n",
    "      if (number == '911'): continue\n",
    "\n",
    "      # Quitando numeros\n",
    "      text = text.replace(number, '')\n",
    "\n",
    "  text = text.replace('utc', '')              # Quita utc\n",
    "  text = removeStepWords(text)                # Quita todas las step words\n",
    "  text = re.sub(' +', ' ', text)              # Quita todos los espacios de mas\n",
    "\n",
    "  cleanText.append(text)\n",
    "\n",
    "train['text'] = cleanText[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['target']\n",
    "train = train.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Exploratorio II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'].value_counts().head(10).plot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a8e74bc410295dd7b3e2a92a04bda485b935e7c35103812674b9cdd1b25ea1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
